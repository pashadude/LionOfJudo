{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judo Throw Recognition with YOLO11 Pose\n",
    "\n",
    "**Free GPU Testing** - Run this notebook on Google Colab\n",
    "\n",
    "This notebook:\n",
    "1. Downloads a test judo video from YouTube\n",
    "2. Runs YOLO11 pose estimation\n",
    "3. Analyzes biomechanics to detect throws\n",
    "4. Shows results with skeleton overlays\n",
    "\n",
    "**Usage:** Click Runtime â†’ Run All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ultralytics yt-dlp opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test video from YouTube\n",
    "# Using one of the technique demonstration videos\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=LMKgaMdm9UY\"  # 80 tachi-waza techniques\n",
    "\n",
    "!yt-dlp -f 'best[height<=720]' -o 'judo_test.mp4' --download-sections \"*0:00-2:00\" {VIDEO_URL}\n",
    "\n",
    "print(\"âœ“ Downloaded 2 minutes of test video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "\n",
    "# Load YOLO11 pose model\n",
    "print(\"Loading YOLO11x-pose model...\")\n",
    "model = YOLO('yolo11x-pose.pt')  # Automatically downloads if not present\n",
    "print(\"âœ“ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pose estimation on video\n",
    "print(\"Running pose estimation...\")\n",
    "\n",
    "results = model.predict(\n",
    "    source='judo_test.mp4',\n",
    "    save=True,\n",
    "    project='output',\n",
    "    name='pose_results',\n",
    "    conf=0.5,\n",
    "    show_labels=True,\n",
    "    show_conf=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Pose estimation complete!\")\n",
    "print(\"Output saved to: output/pose_results/judo_test.avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample frame with pose overlay\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the output video and show a frame\n",
    "cap = cv2.VideoCapture('output/pose_results/judo_test.avi')\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 100)  # Jump to frame 100\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if ret:\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Sample Frame with YOLO11 Pose Overlay')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Could not load frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pose data for analysis\n",
    "print(\"Extracting pose keypoints...\")\n",
    "\n",
    "cap = cv2.VideoCapture('judo_test.mp4')\n",
    "results = model.predict(source='judo_test.mp4', stream=True, verbose=False)\n",
    "\n",
    "pose_data = []\n",
    "for frame_idx, result in enumerate(results):\n",
    "    if result.keypoints is not None and len(result.keypoints.data) > 0:\n",
    "        for person_idx, kpts in enumerate(result.keypoints.data):\n",
    "            keypoints_array = kpts.cpu().numpy()\n",
    "            \n",
    "            pose_data.append({\n",
    "                'frame': frame_idx,\n",
    "                'person': person_idx,\n",
    "                'keypoints': keypoints_array.tolist()\n",
    "            })\n",
    "    \n",
    "    if frame_idx % 100 == 0:\n",
    "        print(f\"  Processed {frame_idx} frames\")\n",
    "\n",
    "cap.release()\n",
    "print(f\"\\nâœ“ Extracted {len(pose_data)} person-frames\")\n",
    "\n",
    "# Save pose data\n",
    "with open('pose_data.json', 'w') as f:\n",
    "    json.dump(pose_data, f)\n",
    "\n",
    "print(\"âœ“ Pose data saved to pose_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple biomechanical analysis\n",
    "# Calculate hip heights over time\n",
    "\n",
    "hip_heights = []\n",
    "frame_numbers = []\n",
    "\n",
    "for item in pose_data:\n",
    "    kpts = np.array(item['keypoints'])\n",
    "    \n",
    "    # COCO keypoints: 11=left_hip, 12=right_hip\n",
    "    left_hip = kpts[11]\n",
    "    right_hip = kpts[12]\n",
    "    \n",
    "    # Check confidence\n",
    "    if left_hip[2] > 0.5 and right_hip[2] > 0.5:\n",
    "        avg_hip_y = (left_hip[1] + right_hip[1]) / 2\n",
    "        hip_heights.append(avg_hip_y)\n",
    "        frame_numbers.append(item['frame'])\n",
    "\n",
    "# Plot hip height over time\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(frame_numbers, hip_heights, linewidth=1)\n",
    "plt.xlabel('Frame Number')\n",
    "plt.ylabel('Hip Height (pixels, inverted - lower=higher)')\n",
    "plt.title('Hip Height Trajectory - Drops indicate potential throws')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.gca().invert_yaxis()  # Invert so drops go down visually\n",
    "plt.show()\n",
    "\n",
    "# Find significant hip drops (potential throws)\n",
    "hip_heights_array = np.array(hip_heights)\n",
    "# Look for local maxima followed by drops\n",
    "window_size = 30\n",
    "throws_detected = []\n",
    "\n",
    "for i in range(len(hip_heights_array) - window_size):\n",
    "    window = hip_heights_array[i:i+window_size]\n",
    "    drop = np.max(window) - np.min(window)\n",
    "    \n",
    "    if drop > 50:  # Significant drop threshold (pixels)\n",
    "        throws_detected.append({\n",
    "            'frame': frame_numbers[i],\n",
    "            'time': frame_numbers[i] / 30,  # Assume 30fps\n",
    "            'drop_amount': drop\n",
    "        })\n",
    "\n",
    "print(f\"\\nâœ“ Detected {len(throws_detected)} potential throws based on hip drops:\")\n",
    "for throw in throws_detected[:10]:  # Show first 10\n",
    "    print(f\"  - Frame {throw['frame']} ({throw['time']:.1f}s): {throw['drop_amount']:.0f}px drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "print(\"Preparing files for download...\")\n",
    "\n",
    "# Compress output video to reduce size\n",
    "!ffmpeg -i output/pose_results/judo_test.avi -c:v libx264 -crf 28 -y judo_pose_annotated.mp4 -loglevel quiet\n",
    "\n",
    "print(\"\\nðŸ“¥ Download these files:\")\n",
    "files.download('judo_pose_annotated.mp4')\n",
    "files.download('pose_data.json')\n",
    "\n",
    "print(\"\\nâœ“ Complete! You now have:\")\n",
    "print(\"  1. judo_pose_annotated.mp4 - Video with skeleton overlays\")\n",
    "print(\"  2. pose_data.json - Raw pose keypoints for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Analyze Quality**: Review the skeleton overlays - are keypoints accurate?\n",
    "2. **Hip Detection**: Check if the hip drop detection correctly identifies throws\n",
    "3. **Fine-tune**: Adjust threshold values for your specific videos\n",
    "4. **Hybrid Approach**: Combine with Vision LLM (Gemini/Claude) for technique classification\n",
    "\n",
    "**Cost Estimate**: This entire notebook runs free on Colab! For production:\n",
    "- Hetzner GPU: ~$0.50/hour\n",
    "- 2-hour video processing: ~10 min = $0.08\n",
    "- Very affordable compared to Vision LLM only ($0.04-0.50/session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
